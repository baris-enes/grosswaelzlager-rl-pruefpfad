{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Reinforcement Learning basierte PrÃ¼fpfadentscheidung bei Offshore-WÃ¤lzlagern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projektziel:\n",
        "\n",
        "### Der Reinforcement-Learning-Agent soll lernen, fÃ¼r ein Offshore-GroÃŸwÃ¤lzlager auf Basis des aktuellen Zustands (Temperaturgradient, WÃ¤rmebehandlungsstatus, Aktionshistorie) eine geeignete PrÃ¼fstrategie auszuwÃ¤hlen. Dabei steht die Entscheidung im Fokus, ob:\n",
        "\n",
        "- keine PrÃ¼fung (Skip),\n",
        "\n",
        "- eine kostengÃ¼nstige Brinell-HÃ¤rteprÃ¼fung oder\n",
        "\n",
        "- eine aufwendige UltraschallprÃ¼fung (UT) erforderlich ist, und ob vorherige ZustandsverÃ¤nderungen (z.â€¯B. Heizen, WÃ¤rmebehandlung) sinnvoll sind, um eine kosteneffizientere PrÃ¼fung zu ermÃ¶glichen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Physikalischer Hintergrund\n",
        "\n",
        "**Werkstoff:** 100Cr6 (1.3505), typischer WÃ¤lzlagerstahl  \n",
        "**HÃ¤rte nach WÃ¤rmebehandlung:** ~**700 HBW** (â‰ˆ 62 HRC, umgerechnet)  \n",
        "**Einsatzbereich:** Offshore-Anwendungen mit **C5-M**-Korrosionsanforderung gemÃ¤ÃŸ **ISO 12944**\n",
        "\n",
        "###  Wichtige Werkstoffmerkmale:\n",
        "- Hohe HÃ¤rte und VerschleiÃŸfestigkeit nach Ã–labschrecken und Anlassen\n",
        "- Nicht korrosionsbestÃ¤ndig â†’ **Chromgehalt < 13â€¯%**\n",
        "- ZusÃ¤tzlicher **Korrosionsschutz erforderlich** (z.â€¯B. Zn/Ni-Beschichtung, Kapselung, Dichtungen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  PrÃ¼fmethoden im Vergleich\n",
        "\n",
        "| Verfahren        | Zeitaufwand      | Eigenschaften                             | Typischer Einsatz                    |\n",
        "|------------------|------------------|--------------------------------------------|--------------------------------------|\n",
        "| **Brinell (HBW)**| ~**3 Minuten**    | mechanisch robust, flÃ¤chige Eindrucksmessung | Standardkontrolle fÃ¼r gehÃ¤rtete OberflÃ¤chen |\n",
        "| **Ultraschall (UT1 + UT2)** | ~**30 Minuten** | fehlerortend, tiefenauflÃ¶send, teurer     | bei Verdacht auf innere Defekte oder Endkontrolle |\n",
        "\n",
        "**Entscheidungskriterium:**  \n",
        "Der Reinforcement-Learning-Agent soll **selbststÃ¤ndig lernen**, ob eine kostengÃ¼nstige Brinell-PrÃ¼fung **ausreicht**, oder ob eine aufwendige UltraschallprÃ¼fung **nÃ¶tig ist** â€“ je nach Zustand des Bauteils.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Thermische Gradientendynamik\n",
        "\n",
        "**Temperaturverteilung innerhalb des GroÃŸwÃ¤lzlagers** beeinflusst die Eigenspannungen und PrÃ¼fentscheidungen:\n",
        "\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "  <img src=\"plots/Temperatur_gradient_weiÃŸ.png\" width=\"450\">\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  WÃ¤rmebehandlungsverlauf\n",
        "\n",
        "Umwandlungen wie Austenit â†’ Martensit oder Bainit folgen temperaturabhÃ¤ngigen Reaktionsraten, modelliert durch die Arrhenius-Gleichung:\n",
        "\n",
        "$$\n",
        "k(T) = A \\cdot \\exp\\left(-\\frac{Q}{R \\cdot T}\\right)\n",
        "$$\n",
        "\n",
        "**Legende:**\n",
        "\n",
        "- \\(k(T)\\): Reaktionsgeschwindigkeit  \n",
        "- \\(A\\): prÃ¤exponentieller Faktor  \n",
        "- \\(Q\\): Aktivierungsenergie [J/mol]  \n",
        "- \\(R = 8,314~{J/molÂ·K}\\): universelle Gaskonstante  \n",
        "- \\(T\\): Temperatur in Kelvin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spielregeln\n",
        "\n",
        "### Zustand\n",
        "\n",
        "- temperaturgradient âˆˆ [0, 1]: simuliert âˆ‡T normiert, kritisch ab ~0.7\n",
        "- wÃ¤rmebehandlungsgrad âˆˆ {0, 1}: 1 = korrekt durchgefÃ¼hrt, 0 = mangelhaft\n",
        "- last_action, prev_action âˆˆ {0â€“5} (Index: [Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "###  AktionsÃ¼bersicht & Rewards\n",
        "\n",
        "| Index | Aktion                  | Effekt                                            | Reward (wenn erlaubt) | Reward (wenn nicht erlaubt)                      |\n",
        "|-------|-------------------------|---------------------------------------------------|------------------------|--------------------------------------------------|\n",
        "| 0     | Skip                    | Keine PrÃ¼fung, beendet Episode                   | 0                      | âˆ’5 (wenn âˆ‡T > 0.7 oder WBH < 0.3)                |\n",
        "| 1     | UT (UltraschallprÃ¼fung) | beendet Episode                                  | +3                     | âˆ’1.5                                             |\n",
        "| 2     | Brinell (HÃ¤rteprÃ¼fung)  | beendet Episode                                  | +2 (wenn WBH â‰¥ 0.3)    | âˆ’1 (wenn WBH < 0.3)                              |\n",
        "| 3     | Heizen                  | âˆ‡T + 0.1, kein done                               | âˆ’0.1                   | â€”                                                |\n",
        "| 4     | KÃ¼hlen                  | âˆ‡T âˆ’ 0.1, WBH âˆ’ 0.1, kein done                    | âˆ’0.1                   | â€”                                                |\n",
        "| 5     | WÃ¤rmebehandlung         | WBH + 0.2, kein done                              | âˆ’0.1                   | â€”                                                |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entscheidungslogik\n",
        "\n",
        "- Wenn `temperaturgradient > 0.7` â†’ **UT ist sicherer**  \n",
        "  (hohe Eigenspannung â†’ Brinell ungeeignet)\n",
        "\n",
        "- Wenn `temperaturgradient < 0.5` **und** `wÃ¤rmebehandlungsgrad â‰¥ 0.3` â†’ **Brinell genÃ¼gt**  \n",
        "  (kosteneffiziente Variante, maximaler Reward)\n",
        "\n",
        "- Wenn `wÃ¤rmebehandlungsgrad < 0.3` â†’ **Brinell nicht erlaubt**  \n",
        "  â†’ Agent kann:\n",
        "  - entweder `UT` durchfÃ¼hren (sicher, aber teuer), **oder**\n",
        "  - vorher `wÃ¤rmebehandeln`, um `wÃ¤rmebehandlungsgrad` auf â‰¥ 0.3 zu bringen und **dann Brinell**\n",
        "\n",
        "- Aktionen wie `Heizen`, `KÃ¼hlen`, `WÃ¤rmebehandlung` dienen dazu, den Zustand gezielt in einen prÃ¼fbaren Bereich zu verschieben  \n",
        "  (z.â€¯B. âˆ‡T erhÃ¶hen fÃ¼r UT, WÃ¤rmebehandlung verbessern fÃ¼r Brinell)\n",
        "\n",
        "- âš ï¸ **Um zu verhindern, dass der Agent durch wiederholtes Heizen kÃ¼nstlich den UT-Zustand erzeugt, um konstant +3 zu kassieren,** wurde eine Regel eingefÃ¼hrt:\n",
        "- **Maximal 2 Aktionen pro Episode**\n",
        "- **Wiederholung derselben Aktion fÃ¼hrt zu einer Strafbewertung von âˆ’1.0**\n",
        "\n",
        "\n",
        "**Ziel des Agenten:**\n",
        "- So **wenig UT wie nÃ¶tig** (teuer, aber sicher)\n",
        "- So **wenig Skip wie mÃ¶glich** (QualitÃ¤tsrisiko)\n",
        "- So **oft wie mÃ¶glich Brinell**, **wenn Zustand es zulÃ¤sst**\n",
        "- **ZustandsverÃ¤nderungen intelligent nutzen**, um gÃ¼nstige PrÃ¼fungen zu ermÃ¶glichen\n",
        "\n",
        "\n",
        " ###  <h3 align=\"center\">Flowchart - Entscheidungslogik</h3>\n",
        " <p align=\"center\">\n",
        "  <img src=\"plots/rl_flowchart_.png\" width=\"450\"/>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Beispielhafte Entscheidungslogik eines intelligenten Agents\n",
        "\n",
        "####  **Szenario 1: Direkter Brinell-Check**\n",
        "- Zustand: `temperaturgradient = 0.3`, `wÃ¤rmebehandlungsgrad = 0.9`\n",
        "- Aktion: `Brinell` (gÃ¼nstig, schnell)\n",
        "- Reward: **+2**\n",
        "\n",
        " Entscheidung: Der Agent erkennt einen idealen Zustand und nutzt die **kostengÃ¼nstige Brinell-PrÃ¼fung**.\n",
        "\n",
        "---\n",
        "\n",
        "####  **Szenario 2: Unsicherer Zustand â†’ Zustand verbessern â†’ Brinell**\n",
        "- Anfangszustand: `temperaturgradient = 0.4`, `wÃ¤rmebehandlungsgrad = 0.1`\n",
        "- Aktionen: Brinell nicht erlaubt, da WÃ¤rmebehandlung die Mindestanforderung von 0.3 nicht erreicht\n",
        "  1. `WÃ¤rmebehandlung` â†’ `wÃ¤rmebehandlungsgrad` steigt auf 0.3 â†’ Reward: **âˆ’0.1**\n",
        "  2. `Brinell` (nun erlaubt) â†’ Reward **+2**\n",
        "\n",
        "- Gesamt-Reward: **+1.9**\n",
        "\n",
        " Entscheidung: Der Agent investiert **einmalig Energie**, um einen Brinell-PrÃ¼fzustand herzustellen. **GÃ¼nstiger als UT.**\n",
        "\n",
        "---\n",
        "\n",
        "####  **Szenario 3: Temperatur zu niedrig â†’ Agent heizt â†’ dann UT**\n",
        "- Anfangszustand: `temperaturgradient = 0.4`, `wÃ¤rmebehandlungsgrad = 0.8`\n",
        "- Aktionen:\n",
        "  1. `Heizen` â†’ `temperaturgradient = 0.5` â†’ Reward: **âˆ’0.1**\n",
        "  2. `UT` â†’ Reward: **+3**\n",
        "\n",
        "- Gesamt-Reward: **+2.9**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Ãœberblick der RL-Umgebungen (Environments)\n",
        "\n",
        "### ğŸŸ¡ `WzlPruefEnv2D` \n",
        "- Zustandsraum:\n",
        "`temperaturgradient` (z.â€¯B. innen â†” auÃŸen)\n",
        "`wÃ¤rmebehandlungsgrad` (QualitÃ¤t des WÃ¤rmeprozesses)\n",
        "- `state = (temperaturgradient, wÃ¤rmebehandlungsgrad)`\n",
        "\n",
        "\n",
        "- Ziel:\n",
        "Lerne einfache PrÃ¼fentscheidungen basierend auf aktuellem Zustand.\n",
        "\n",
        "- Typische Strategie:\n",
        "â†’ â€Wenn gradient hoch â†’ UT; wenn Behandlung schlecht â†’ Brinellâ€œ\n",
        "\n",
        "- Einsatz:\n",
        "Baseline-Agenten wie SimpleAgentV1, erstes Q-Learning (V1)\n",
        "\n",
        "\n",
        "### ğŸŸ  `WzlPruefEnv3D`\n",
        "- ZusÃ¤tzlicher Zustand:\n",
        "last_action â€“ Was wurde in der vorherigen Aktion gemacht?\n",
        "- `state = (temperaturgradient, wÃ¤rmebehandlungsgrad, last_action)`\n",
        "\n",
        "- Ziel:\n",
        "Modellieren von Verkettungen wie:\n",
        "â€Heizen â†’ anschlieÃŸend Brinellâ€œ oder â€WÃ¤rmebehandlung â†’ UTâ€œ\n",
        "\n",
        "- Warum wichtig?\n",
        "PrÃ¼fpfade sind oft mehrstufig, nicht isolierte Einzelschritte.\n",
        "\n",
        "- Einsatz:\n",
        "QLearningAgentV2 (mit rudimentÃ¤rem Entscheidungsverlauf)\n",
        "\n",
        "### ğŸ”µ`WzlPruefEnv4D`\n",
        "- ZusÃ¤tzlicher Zustand:\n",
        "prev_action â€“ Zwei Aktionen zurÃ¼ck (wie ein kurzes GedÃ¤chtnis)\n",
        "- `state = (temperaturgradient, wÃ¤rmebehandlungsgrad, last_action, prev_action)`\n",
        "\n",
        "- Ziel:\n",
        "RÃ¼ckkopplungseffekte, PrÃ¼fpfad-Optimierung Ã¼ber mehrere Schritte hinweg\n",
        "\n",
        "- Herausforderung:\n",
        "10Ã—10Ã—6Ã—6 ZustÃ¤nde = 3600 EintrÃ¤ge â†’ Q-Table stÃ¶ÃŸt an Grenzen\n",
        "\n",
        "- Einsatz:\n",
        " DQN â€“ notwendig fÃ¼r tieferes EntscheidungsverstÃ¤ndnis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Agentenentwicklung â€“ Evolution der Strategien\n",
        "\n",
        "###  SimpleAgentV1\n",
        "\n",
        "- **Typ:** Regelbasierter Agent ohne Zustandsverfolgung  \n",
        "- **Zustand:** `(Temperaturgradient, WÃ¤rmebehandlungsgrad)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  \n",
        "- **Reward:** Ã˜ ca. **0.84**\n",
        "- **Erkenntnis:**  \n",
        "  - Klare Entscheidungsregeln schlagen teilweise explorative Lernmethoden  \n",
        "  - Diente als robuster Benchmark fÃ¼r spÃ¤tere Agenten\n",
        "\n",
        "  ###  <h3 align=\"center\">Lernverlauf â€“ SimpleAgentV1</h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"plots/SimpleAgentV1_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "###  QLearningAgentV1 (2D)\n",
        "\n",
        "- **Zustand:** `(Temperaturgradient, WÃ¤rmebehandlungsgrad)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  \n",
        "- **Reward:** Ã˜ bis **~1.43**  \n",
        "- **Besonderheit:**  \n",
        "  - Klassischer Q-Learning Agent mit tabellarischer ReprÃ¤sentation  \n",
        "  - Grid Search Ã¼ber Î±, Î³, Îµ_min und decay  \n",
        "  - Visualisierung der besten Kombinationen (Top 1â€“3â€“5â€“10) mit Moving Average\n",
        "\n",
        "- **Erkenntnisse:**  \n",
        "  - Grid Search zeigte starke HyperparameterabhÃ¤ngigkeit  \n",
        "  - Einfache Umgebung â†’ schneller Lerneffekt\n",
        "\n",
        "  ### <h3 align=\"center\">Lernverlauf â€“ QLearningAgentV1 (2D)</h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"plots/QLearningAgentV1_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "###  QLearningAgentV2 (3D)\n",
        "\n",
        "- **Zustand:** `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  \n",
        "- **Reward:** Ã˜ bis **~1.55**  \n",
        "- **Ziel:** Modellierung einfacher Sequenzen wie â€Heizen â†’ Brinellâ€œ  \n",
        "- **Besonderheit:**  \n",
        "  - Zustandserweiterung um letzte Aktion  \n",
        "  - 600 mÃ¶gliche ZustÃ¤nde\n",
        "\n",
        "- **Erkenntnisse:**  \n",
        "  - Verbesserte Leistung gegenÃ¼ber QLearningAgentV1  \n",
        "  - Historische Kontextinformation fÃ¼hrt zu gezielteren Entscheidungen  \n",
        "  - Erste Anzeichen steigender KomplexitÃ¤t â†’ Performance stagniert ab gewissen Grenzen\n",
        "\n",
        "  ### <h3 align=\"center\">Lernverlauf â€“ QLearningAgentV2 (3D)</h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"plots/QLearningAgentV2_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "###  DQN (4D, mit neuronalen Netzen)\n",
        "\n",
        "- **Zustand:** `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action, prev_action)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  \n",
        "- **Reward:** Plateau bei **2.03**, auch nach 200k Episoden  \n",
        "- **Besonderheit:**  \n",
        "  - Deep Q-Network auf Basis von TensorFlow  \n",
        "  - 3600 mÃ¶gliche ZustÃ¤nde â†’ tabellarisches Q-Learning nicht mehr sinnvoll   \n",
        "  - Speichert Rewards automatisch in `.npy`-Dateien â†’ resume-fÃ¤hig\n",
        "\n",
        "- **Erkenntnisse:**  \n",
        "  - Klassisches Q-Learning skaliert schlecht in hohen Dimensionen  \n",
        "  - DQN ermÃ¶glicht bessere Verallgemeinerung, aber Reward bleibt limitiert  \n",
        "  - Hauptlimitierender Faktor: das physikalisch definierte Reward-System\n",
        "\n",
        "  ### <h3 align=\"center\">Lernverlauf â€“ DQNAgent (4D) </h3> \n",
        "  <p align=\"center\">\n",
        "  <img src=\"plots/DQNAgent_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[def]: lots/SimpleAgentV1_.pn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Projekttechnik\n",
        "\n",
        "- **Grid Search** Ã¼ber >50 Hyperparameterkombinationen (`Î±`, `Î³`, `Îµ_min`, `decay`)\n",
        "- **Automatische Zwischenspeicherung** aller Trainingskurven (`.npy`) fÃ¼r Reproduzierbarkeit\n",
        "- **Logging**: Durchschnittlicher Reward alle 1000 Episoden zur Verlaufskontrolle\n",
        "- **Visualisierung** der Top-N-Strategien mit:\n",
        "  - **Moving Average** (FenstergrÃ¶ÃŸe: 100)\n",
        "  - **Farbkodierung** (Top 1â€“3: blau, rot, orange)\n",
        "  - **Export** als `.csv` und `.png` fÃ¼r weitere Analyse\n",
        "- **DQN-Modul** (Deep Q-Network) fÃ¼r Zustandsverarbeitung mit neuronalen Netzen\n",
        "â†’ Anwendung von Replay Buffer, Îµ-greedy Policy, Target Network\n",
        "\n",
        "###  Skalierung der RL-Umgebungen\n",
        "\n",
        "Mit jeder Agentengeneration wurde gezielt die **Zustandsdimensionierung erhÃ¶ht**:\n",
        "\n",
        "- `2D`: Basiszustand  \n",
        "  â†’ `(Temperaturgradient, wÃ¤rmebehandlungsgrad)`\n",
        "- `3D`: zusÃ¤tzlicher Kontext Ã¼ber letzte Aktion  \n",
        "  â†’ `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action)`\n",
        "- `4D`: komplette Entscheidungsfolge Ã¼ber zwei Zeitschritte  \n",
        "  â†’ `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action, prev_action)`\n",
        "\n",
        " **Ziel**: Bewertung, wie weit klassische RL-Verfahren (Q-Table, DQN) mit wachsender ZustandskomplexitÃ¤t skalieren.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "##  Verzeichnisstruktur\n",
        "\n",
        "```plaintext\n",
        "grosswaelzlager_pruef_rl/\n",
        "â”œâ”€â”€ README.ipynb                  â† Hauptdokumentation mit RL- und Werkstoffwissen\n",
        "â”œâ”€â”€ requirements.txt              â† AbhÃ¤ngigkeiten (pip install)\n",
        "â”œâ”€â”€ plots/                        â† Trainingskurven & Flowcharts\n",
        "â”‚   â”œâ”€â”€ DQNAgent_.png\n",
        "â”‚   â”œâ”€â”€ QLearningAgentV1_.png\n",
        "â”‚   â”œâ”€â”€ QLearningAgentV2_.png\n",
        "â”‚   â”œâ”€â”€ SimpleAgentV1_.png\n",
        "â”‚   â””â”€â”€ rl_flowchart_.png\n",
        "â”œâ”€â”€ environments/                â† Simulationsumgebungen (2D, 3D, 4D)\n",
        "â”‚   â”œâ”€â”€ env_wzl_0.py             2D\n",
        "â”‚   â”œâ”€â”€ env_wzl_1.py             3D\n",
        "â”‚   â””â”€â”€ env_wzl_2.py             4D\n",
        "â”œâ”€â”€ rl_agent/                    â† Verschiedene Agenten-Implementierungen\n",
        "â”‚   â”œâ”€â”€ agent_simple_v1.py\n",
        "â”‚   â”œâ”€â”€ DQNAgent.py\n",
        "â”‚   â”œâ”€â”€ q_learning_agent_V1.py\n",
        "â”‚   â””â”€â”€ q_learning_agent_V2.py\n",
        "â”œâ”€â”€ notebooks/                   â† Auswertung und Visualisierung\n",
        "â”‚   â””â”€â”€ agenten_plots.ipynb\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "##  Fazit & nÃ¤chste Schritte\n",
        "\n",
        "- Klassische **Q-Learning-Tabellen** reichen fÃ¼r einfache, klar strukturierte PrÃ¼fentscheidungen aus  \n",
        "- In **hochdimensionalen ZustandsrÃ¤umen**,  (z.â€¯B. `env_wzl_2 - 4D`) ist eine **Deep-Q-Network-Architektur (DQN)** notwendig  \n",
        "- Die Kombination aus **DomÃ¤nenwissen** (_z.â€¯B. Gradientformel, Werkstoffverhalten_) und datengetriebenem Lernen liefert robuste PrÃ¼fstrategien  \n",
        "  Nach 200.000 Episoden erreichte er einen stabilen durchschnittlichen Reward von   **Ã˜ 2.05**.\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "###  Industrie 4.0 â€“ SAP-Anbindung (Optionaler Ausblick)\n",
        "\n",
        "- Ãœber ein SAP S/4HANA-System lassen sich produktionsnahe Parameter wie WÃ¤rmebehandlungsstatus, PrÃ¼fauftragsdaten oder Materialchargen automatisiert bereitstellen\n",
        "- Das RL-Modell kann so auf **Live-Daten** reagieren und direkt in **QualitÃ¤tsentscheidungen** integriert werden (z.â€¯B. Ã¼ber das Modul **SAP QM** oder **SAP PP**)\n",
        "- Einbindung Ã¼ber **OData- oder REST-API** wÃ¤re technisch realisierbar (z.â€¯B. via Python-Schnittstelle zu SAP Gateway)\n",
        "- Produktionssysteme wie **MES** (Manufacturing Execution Systems) oder **BDE** (Betriebsdatenerfassung) kÃ¶nnten als praxisnahe Schnittstelle zur RL-Integration dienen.\n",
        "\n",
        "\n",
        "**Vorteil:**  \n",
        "Reale Produktionsdaten flieÃŸen in adaptive PrÃ¼fstrategien â†’ vollautomatisiertes PrÃ¼fsystem mit RÃ¼ckkopplung\n",
        "\n",
        "---\n",
        "\n",
        "**NÃ¤chster Schritt:**  \n",
        " Integration eines DQN mit Convolutional/Linear Layers zur **Generalisation Ã¼ber ZustandsrÃ¤ume hinweg**\n",
        "\n",
        "**Langfristige Perspektive:**\n",
        "- Einbindung von Sensordaten (z.â€¯B. reale TemperaturverlÃ¤ufe, Korrosionsraten)  \n",
        "- SAP-Datenanbindung zur intelligenten PrÃ¼fsteuerung basierend auf Echtzeitdaten  \n",
        "- Einsatz von Transfer Learning, um Wissen von einem Lagertyp auf andere Varianten zu Ã¼bertragen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  Kontakt & Profile\n",
        "\n",
        "\n",
        "-  LinkedIn (https://www.linkedin.com/in/baris-enes)\n",
        "-  GitHub: (https://github.com/baris-enes)\n",
        "\n",
        "---\n",
        "\n",
        "*Dieses Projekt wurde im Rahmen meiner Spezialisierung auf Reinforcement Learning fÃ¼r die industriellen QualitÃ¤tssicherung entwickelt â€“ mit besonderem Fokus auf physikalisch fundierte Entscheidungsmodelle und SAP-nahe Produktionsdaten.*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3_11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
