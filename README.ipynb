{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Reinforcement Learning basierte Pr√ºfpfadentscheidung bei Offshore-W√§lzlagern"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Projektziel:\n",
        "\n",
        "### Der Reinforcement-Learning-Agent soll lernen, f√ºr ein Offshore-Gro√üw√§lzlager auf Basis des aktuellen Zustands (Temperaturgradient, W√§rmebehandlungsstatus, Aktionshistorie) eine geeignete Pr√ºfstrategie auszuw√§hlen. Dabei steht die Entscheidung im Fokus, ob:\n",
        "\n",
        "- keine Pr√ºfung (Skip),\n",
        "\n",
        "- eine kosteng√ºnstige Brinell-H√§rtepr√ºfung oder\n",
        "\n",
        "- eine aufwendige Ultraschallpr√ºfung (UT) erforderlich ist, und ob vorherige Zustandsver√§nderungen (z.‚ÄØB. Heizen, W√§rmebehandlung) sinnvoll sind, um eine kosteneffizientere Pr√ºfung zu erm√∂glichen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Physikalischer Hintergrund\n",
        "\n",
        "**Werkstoff:** 100Cr6 (1.3505), typischer W√§lzlagerstahl  \n",
        "**H√§rte nach W√§rmebehandlung:** ~**700 HBW** (‚âà 62 HRC, umgerechnet)  \n",
        "**Einsatzbereich:** Offshore-Anwendungen mit **C5-M**-Korrosionsanforderung gem√§√ü **ISO 12944**\n",
        "\n",
        "###  Wichtige Werkstoffmerkmale:\n",
        "- Hohe H√§rte und Verschlei√üfestigkeit nach √ñlabschrecken und Anlassen\n",
        "- Nicht korrosionsbest√§ndig ‚Üí **Chromgehalt < 13‚ÄØ%**\n",
        "- Zus√§tzlicher **Korrosionsschutz erforderlich** (z.‚ÄØB. Zn/Ni-Beschichtung, Kapselung, Dichtungen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Pr√ºfmethoden im Vergleich\n",
        "\n",
        "| Verfahren        | Zeitaufwand      | Eigenschaften                             | Typischer Einsatz                    |\n",
        "|------------------|------------------|--------------------------------------------|--------------------------------------|\n",
        "| **Brinell (HBW)**| ~**3 Minuten**    | mechanisch robust, fl√§chige Eindrucksmessung | Standardkontrolle f√ºr geh√§rtete Oberfl√§chen |\n",
        "| **Ultraschall (UT1 + UT2)** | ~**30 Minuten** | fehlerortend, tiefenaufl√∂send, teurer     | bei Verdacht auf innere Defekte oder Endkontrolle |\n",
        "\n",
        "**Entscheidungskriterium:**  \n",
        "Der Reinforcement-Learning-Agent soll **selbstst√§ndig lernen**, ob eine kosteng√ºnstige Brinell-Pr√ºfung **ausreicht**, oder ob eine aufwendige Ultraschallpr√ºfung **n√∂tig ist** ‚Äì je nach Zustand des Bauteils.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Thermische Gradientendynamik\n",
        "\n",
        "**Temperaturverteilung innerhalb des Gro√üw√§lzlagers** beeinflusst die Eigenspannungen und Pr√ºfentscheidungen:\n",
        "\n",
        "\n",
        "<div style=\"text-align: center\">\n",
        "  <img src=\"plots/Temperatur_gradient_wei√ü.png\" width=\"450\">\n",
        "</div>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  W√§rmebehandlungsverlauf\n",
        "\n",
        "Umwandlungen wie Austenit ‚Üí Martensit oder Bainit folgen temperaturabh√§ngigen Reaktionsraten, modelliert durch die Arrhenius-Gleichung:\n",
        "\n",
        "$$\n",
        "k(T) = A \\cdot \\exp\\left(-\\frac{Q}{R \\cdot T}\\right)\n",
        "$$\n",
        "\n",
        "**Legende:**\n",
        "\n",
        "- \\(k(T)\\): Reaktionsgeschwindigkeit  \n",
        "- \\(A\\): pr√§exponentieller Faktor  \n",
        "- \\(Q\\): Aktivierungsenergie [J/mol]  \n",
        "- \\(R = 8,314~{J/mol¬∑K}\\): universelle Gaskonstante  \n",
        "- \\(T\\): Temperatur in Kelvin\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spielregeln\n",
        "\n",
        "### Zustand\n",
        "\n",
        "- temperaturgradient ‚àà [0, 1]: simuliert ‚àáT normiert, kritisch ab ~0.7\n",
        "- w√§rmebehandlungsgrad ‚àà {0, 1}: 1 = korrekt durchgef√ºhrt, 0 = mangelhaft\n",
        "- last_action, prev_action ‚àà {0‚Äì5} (Index: [Skip, UT, Brinell, Heizen, K√ºhlen, W√§rmebehandlung])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "###  Aktions√ºbersicht & Rewards\n",
        "\n",
        "| Index | Aktion                  | Effekt                                            | Reward (wenn erlaubt) | Reward (wenn nicht erlaubt)                      |\n",
        "|-------|-------------------------|---------------------------------------------------|------------------------|--------------------------------------------------|\n",
        "| 0     | Skip                    | Keine Pr√ºfung, beendet Episode                   | 0                      | ‚àí5 (wenn ‚àáT > 0.7 oder WBH < 0.3)                |\n",
        "| 1     | UT (Ultraschallpr√ºfung) | beendet Episode                                  | +3                     | ‚àí1.5                                             |\n",
        "| 2     | Brinell (H√§rtepr√ºfung)  | beendet Episode                                  | +2 (wenn WBH ‚â• 0.3)    | ‚àí1 (wenn WBH < 0.3)                              |\n",
        "| 3     | Heizen                  | ‚àáT + 0.1, kein done                               | ‚àí0.1                   | ‚Äî                                                |\n",
        "| 4     | K√ºhlen                  | ‚àáT ‚àí 0.1, WBH ‚àí 0.1, kein done                    | ‚àí0.1                   | ‚Äî                                                |\n",
        "| 5     | W√§rmebehandlung         | WBH + 0.2, kein done                              | ‚àí0.1                   | ‚Äî                                                |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Entscheidungslogik\n",
        "\n",
        "- Wenn `temperaturgradient > 0.7` ‚Üí **UT ist sicherer**  \n",
        "  (hohe Eigenspannung ‚Üí Brinell ungeeignet)\n",
        "\n",
        "- Wenn `temperaturgradient < 0.5` **und** `w√§rmebehandlungsgrad ‚â• 0.3` ‚Üí **Brinell gen√ºgt**  \n",
        "  (kosteneffiziente Variante, maximaler Reward)\n",
        "\n",
        "- Wenn `w√§rmebehandlungsgrad < 0.3` ‚Üí **Brinell nicht erlaubt**  \n",
        "  ‚Üí Agent kann:\n",
        "  - entweder `UT` durchf√ºhren (sicher, aber teuer), **oder**\n",
        "  - vorher `w√§rmebehandeln`, um `w√§rmebehandlungsgrad` auf ‚â• 0.3 zu bringen und **dann Brinell**\n",
        "\n",
        "- Aktionen wie `Heizen`, `K√ºhlen`, `W√§rmebehandlung` dienen dazu, den Zustand gezielt in einen pr√ºfbaren Bereich zu verschieben  \n",
        "  (z.‚ÄØB. ‚àáT erh√∂hen f√ºr UT, W√§rmebehandlung verbessern f√ºr Brinell)\n",
        "\n",
        "- ‚ö†Ô∏è **Um zu verhindern, dass der Agent durch wiederholtes Heizen k√ºnstlich den UT-Zustand erzeugt, um konstant +3 zu kassieren,** wurde eine Regel eingef√ºhrt:\n",
        "- **Maximal 2 Aktionen pro Episode**\n",
        "- **Wiederholung derselben Aktion f√ºhrt zu einer Strafbewertung von ‚àí1.0**\n",
        "\n",
        "\n",
        "**Ziel des Agenten:**\n",
        "- So **wenig UT wie n√∂tig** (teuer, aber sicher)\n",
        "- So **wenig Skip wie m√∂glich** (Qualit√§tsrisiko)\n",
        "- So **oft wie m√∂glich Brinell**, **wenn Zustand es zul√§sst**\n",
        "- **Zustandsver√§nderungen intelligent nutzen**, um g√ºnstige Pr√ºfungen zu erm√∂glichen\n",
        "\n",
        "\n",
        " ###  <h3 align=\"center\">Flowchart - Entscheidungslogik</h3>\n",
        " <p align=\"center\">\n",
        "  <img src=\"plots/rl_flowchart_.png\" width=\"450\"/>\n",
        "</p>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Beispielhafte Entscheidungslogik eines intelligenten Agents\n",
        "\n",
        "####  **Szenario 1: Direkter Brinell-Check**\n",
        "- Zustand: `temperaturgradient = 0.3`, `w√§rmebehandlungsgrad = 0.9`\n",
        "- Aktion: `Brinell` (g√ºnstig, schnell)\n",
        "- Reward: **+2**\n",
        "\n",
        " Entscheidung: Der Agent erkennt einen idealen Zustand und nutzt die **kosteng√ºnstige Brinell-Pr√ºfung**.\n",
        "\n",
        "---\n",
        "\n",
        "####  **Szenario 2: Unsicherer Zustand ‚Üí Zustand verbessern ‚Üí Brinell**\n",
        "- Anfangszustand: `temperaturgradient = 0.4`, `w√§rmebehandlungsgrad = 0.1`\n",
        "- Aktionen: Brinell nicht erlaubt, da W√§rmebehandlung die Mindestanforderung von 0.3 nicht erreicht\n",
        "  1. `W√§rmebehandlung` ‚Üí `w√§rmebehandlungsgrad` steigt auf 0.3 ‚Üí Reward: **‚àí0.1**\n",
        "  2. `Brinell` (nun erlaubt) ‚Üí Reward **+2**\n",
        "\n",
        "- Gesamt-Reward: **+1.9**\n",
        "\n",
        " Entscheidung: Der Agent investiert **einmalig Energie**, um einen Brinell-Pr√ºfzustand herzustellen. **G√ºnstiger als UT.**\n",
        "\n",
        "---\n",
        "\n",
        "####  **Szenario 3: Temperatur zu niedrig ‚Üí Agent heizt ‚Üí dann UT**\n",
        "- Anfangszustand: `temperaturgradient = 0.4`, `w√§rmebehandlungsgrad = 0.8`\n",
        "- Aktionen:\n",
        "  1. `Heizen` ‚Üí `temperaturgradient = 0.5` ‚Üí Reward: **‚àí0.1**\n",
        "  2. `UT` ‚Üí Reward: **+3**\n",
        "\n",
        "- Gesamt-Reward: **+2.9**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  √úberblick der RL-Umgebungen (Environments)\n",
        "\n",
        "### üü° `WzlPruefEnv2D` \n",
        "- Zustandsraum:\n",
        "`temperaturgradient` (z.‚ÄØB. innen ‚Üî au√üen)\n",
        "`w√§rmebehandlungsgrad` (Qualit√§t des W√§rmeprozesses)\n",
        "- `state = (temperaturgradient, w√§rmebehandlungsgrad)`\n",
        "\n",
        "\n",
        "- Ziel:\n",
        "Lerne einfache Pr√ºfentscheidungen basierend auf aktuellem Zustand.\n",
        "\n",
        "- Typische Strategie:\n",
        "‚Üí ‚ÄûWenn gradient hoch ‚Üí UT; wenn Behandlung schlecht ‚Üí Brinell‚Äú\n",
        "\n",
        "- Einsatz:\n",
        "Baseline-Agenten wie SimpleAgentV1, erstes Q-Learning (V1)\n",
        "\n",
        "\n",
        "### üü† `WzlPruefEnv3D`\n",
        "- Zus√§tzlicher Zustand:\n",
        "last_action ‚Äì Was wurde in der vorherigen Aktion gemacht?\n",
        "- `state = (temperaturgradient, w√§rmebehandlungsgrad, last_action)`\n",
        "\n",
        "- Ziel:\n",
        "Modellieren von Verkettungen wie:\n",
        "‚ÄûHeizen ‚Üí anschlie√üend Brinell‚Äú oder ‚ÄûW√§rmebehandlung ‚Üí UT‚Äú\n",
        "\n",
        "- Warum wichtig?\n",
        "Pr√ºfpfade sind oft mehrstufig, nicht isolierte Einzelschritte.\n",
        "\n",
        "- Einsatz:\n",
        "QLearningAgentV2 (mit rudiment√§rem Entscheidungsverlauf)\n",
        "\n",
        "### üîµ`WzlPruefEnv4D`\n",
        "- Zus√§tzlicher Zustand:\n",
        "prev_action ‚Äì Zwei Aktionen zur√ºck (wie ein kurzes Ged√§chtnis)\n",
        "- `state = (temperaturgradient, w√§rmebehandlungsgrad, last_action, prev_action)`\n",
        "\n",
        "- Ziel:\n",
        "R√ºckkopplungseffekte, Pr√ºfpfad-Optimierung √ºber mehrere Schritte hinweg\n",
        "\n",
        "- Herausforderung:\n",
        "10√ó10√ó6√ó6 Zust√§nde = 3600 Eintr√§ge ‚Üí Q-Table st√∂√üt an Grenzen\n",
        "\n",
        "- Einsatz:\n",
        " DQN ‚Äì notwendig f√ºr tieferes Entscheidungsverst√§ndnis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Agentenentwicklung ‚Äì Evolution der Strategien\n",
        "\n",
        "###  SimpleAgentV1\n",
        "\n",
        "- **Typ:** Regelbasierter Agent ohne Zustandsverfolgung  \n",
        "- **Zustand:** `(Temperaturgradient, W√§rmebehandlungsgrad)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, K√ºhlen, W√§rmebehandlung)  \n",
        "- **Reward:** √ò ca. **0.84**\n",
        "- **Erkenntnis:**  \n",
        "  - Klare Entscheidungsregeln schlagen teilweise explorative Lernmethoden  \n",
        "  - Diente als robuster Benchmark f√ºr sp√§tere Agenten\n",
        "\n",
        "  ###  <h3 align=\"center\">Lernverlauf ‚Äì SimpleAgentV1</h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"plots/SimpleAgentV1_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "###  QLearningAgentV1 (2D)\n",
        "\n",
        "- **Zustand:** `(Temperaturgradient, W√§rmebehandlungsgrad)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, K√ºhlen, W√§rmebehandlung)  \n",
        "- **Reward:** √ò bis **~1.43**  \n",
        "- **Besonderheit:**  \n",
        "  - Klassischer Q-Learning Agent mit tabellarischer Repr√§sentation  \n",
        "  - Grid Search √ºber Œ±, Œ≥, Œµ_min und decay  \n",
        "  - Visualisierung der besten Kombinationen (Top 1‚Äì3‚Äì5‚Äì10) mit Moving Average\n",
        "\n",
        "- **Erkenntnisse:**  \n",
        "  - Grid Search zeigte starke Hyperparameterabh√§ngigkeit  \n",
        "  - Einfache Umgebung ‚Üí schneller Lerneffekt\n",
        "\n",
        "  ### <h3 align=\"center\">Lernverlauf ‚Äì QLearningAgentV1 (2D)</h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"plots/QLearningAgentV1_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "###  QLearningAgentV2 (3D)\n",
        "\n",
        "- **Zustand:** `(Temperaturgradient, w√§rmebehandlungsgrad, last_action)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, K√ºhlen, W√§rmebehandlung)  \n",
        "- **Reward:** √ò bis **~1.55**  \n",
        "- **Ziel:** Modellierung einfacher Sequenzen wie ‚ÄûHeizen ‚Üí Brinell‚Äú  \n",
        "- **Besonderheit:**  \n",
        "  - Zustandserweiterung um letzte Aktion  \n",
        "  - 600 m√∂gliche Zust√§nde\n",
        "\n",
        "- **Erkenntnisse:**  \n",
        "  - Verbesserte Leistung gegen√ºber QLearningAgentV1  \n",
        "  - Historische Kontextinformation f√ºhrt zu gezielteren Entscheidungen  \n",
        "  - Erste Anzeichen steigender Komplexit√§t ‚Üí Performance stagniert ab gewissen Grenzen\n",
        "\n",
        "  ### <h3 align=\"center\">Lernverlauf ‚Äì QLearningAgentV2 (3D)</h3>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"plots/QLearningAgentV2_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "###  DQN (4D, mit neuronalen Netzen)\n",
        "\n",
        "- **Zustand:** `(Temperaturgradient, w√§rmebehandlungsgrad, last_action, prev_action)`  \n",
        "- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, K√ºhlen, W√§rmebehandlung)  \n",
        "- **Reward:** Plateau bei **2.03**, auch nach 200k Episoden  \n",
        "- **Besonderheit:**  \n",
        "  - Deep Q-Network auf Basis von TensorFlow  \n",
        "  - 3600 m√∂gliche Zust√§nde ‚Üí tabellarisches Q-Learning nicht mehr sinnvoll   \n",
        "  - Speichert Rewards automatisch in `.npy`-Dateien ‚Üí resume-f√§hig\n",
        "\n",
        "- **Erkenntnisse:**  \n",
        "  - Klassisches Q-Learning skaliert schlecht in hohen Dimensionen  \n",
        "  - DQN erm√∂glicht bessere Verallgemeinerung, aber Reward bleibt limitiert  \n",
        "  - Hauptlimitierender Faktor: das physikalisch definierte Reward-System\n",
        "\n",
        "  ### <h3 align=\"center\">Lernverlauf ‚Äì DQNAgent (4D) </h3> \n",
        "  <p align=\"center\">\n",
        "  <img src=\"plots/DQNAgent_.png\" width=\"800\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[def]: lots/SimpleAgentV1_.pn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Projekttechnik\n",
        "\n",
        "- **Grid Search** √ºber >50 Hyperparameterkombinationen (`Œ±`, `Œ≥`, `Œµ_min`, `decay`)\n",
        "- **Automatische Zwischenspeicherung** aller Trainingskurven (`.npy`) f√ºr Reproduzierbarkeit\n",
        "- **Logging**: Durchschnittlicher Reward alle 1000 Episoden zur Verlaufskontrolle\n",
        "- **Visualisierung** der Top-N-Strategien mit:\n",
        "  - **Moving Average** (Fenstergr√∂√üe: 100)\n",
        "  - **Farbkodierung** (Top 1‚Äì3: blau, rot, orange)\n",
        "  - **Export** als `.csv` und `.png` f√ºr weitere Analyse\n",
        "- **DQN-Modul** (Deep Q-Network) f√ºr Zustandsverarbeitung mit neuronalen Netzen\n",
        "‚Üí Anwendung von Replay Buffer, Œµ-greedy Policy, Target Network\n",
        "\n",
        "###  Skalierung der RL-Umgebungen\n",
        "\n",
        "Mit jeder Agentengeneration wurde gezielt die **Zustandsdimensionierung erh√∂ht**:\n",
        "\n",
        "- `2D`: Basiszustand  \n",
        "  ‚Üí `(Temperaturgradient, w√§rmebehandlungsgrad)`\n",
        "- `3D`: zus√§tzlicher Kontext √ºber letzte Aktion  \n",
        "  ‚Üí `(Temperaturgradient, w√§rmebehandlungsgrad, last_action)`\n",
        "- `4D`: komplette Entscheidungsfolge √ºber zwei Zeitschritte  \n",
        "  ‚Üí `(Temperaturgradient, w√§rmebehandlungsgrad, last_action, prev_action)`\n",
        "\n",
        " **Ziel**: Bewertung, wie weit klassische RL-Verfahren (Q-Table, DQN) mit wachsender Zustandskomplexit√§t skalieren.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "##  Verzeichnisstruktur\n",
        "\n",
        "```plaintext\n",
        "grosswaelzlager_pruef_rl/\n",
        "‚îú‚îÄ‚îÄ README.ipynb                  ‚Üê Hauptdokumentation mit RL- und Werkstoffwissen\n",
        "‚îú‚îÄ‚îÄ requirements.txt              ‚Üê Abh√§ngigkeiten (pip install)\n",
        "‚îú‚îÄ‚îÄ plots/                        ‚Üê Trainingskurven & Flowcharts\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ DQNAgent_.png\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ QLearningAgentV1_.png\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ QLearningAgentV2_.png\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ SimpleAgentV1_.png\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ rl_flowchart_.png\n",
        "‚îú‚îÄ‚îÄ environments/                ‚Üê Simulationsumgebungen (2D, 3D, 4D)\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ env_wzl_0.py             2D\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ env_wzl_1.py             3D\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ env_wzl_2.py             4D\n",
        "‚îú‚îÄ‚îÄ rl_agent/                    ‚Üê Verschiedene Agenten-Implementierungen\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ agent_simple_v1.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ DQNAgent.py\n",
        "‚îÇ   ‚îú‚îÄ‚îÄ q_learning_agent_V1.py\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ q_learning_agent_V2.py\n",
        "‚îú‚îÄ‚îÄ notebooks/                   ‚Üê Auswertung und Visualisierung\n",
        "‚îÇ   ‚îî‚îÄ‚îÄ agenten_plots.ipynb\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "##  Fazit & n√§chste Schritte\n",
        "\n",
        "- Klassische **Q-Learning-Tabellen** reichen f√ºr einfache, klar strukturierte Pr√ºfentscheidungen aus  \n",
        "- In **hochdimensionalen Zustandsr√§umen**,  (z.‚ÄØB. `env_wzl_2 - 4D`) ist eine **Deep-Q-Network-Architektur (DQN)** notwendig  \n",
        "- Die Kombination aus **Dom√§nenwissen** (_z.‚ÄØB. Gradientformel, Werkstoffverhalten_) und datengetriebenem Lernen liefert robuste Pr√ºfstrategien  \n",
        "  Nach 200.000 Episoden erreichte er einen stabilen durchschnittlichen Reward von   **√ò 2.05**.\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "###  Industrie 4.0 ‚Äì SAP-Anbindung (Optionaler Ausblick)\n",
        "\n",
        "- √úber ein SAP S/4HANA-System lassen sich produktionsnahe Parameter wie W√§rmebehandlungsstatus, Pr√ºfauftragsdaten oder Materialchargen automatisiert bereitstellen\n",
        "- Das RL-Modell kann so auf **Live-Daten** reagieren und direkt in **Qualit√§tsentscheidungen** integriert werden (z.‚ÄØB. √ºber das Modul **SAP QM** oder **SAP PP**)\n",
        "- Einbindung √ºber **OData- oder REST-API** w√§re technisch realisierbar (z.‚ÄØB. via Python-Schnittstelle zu SAP Gateway)\n",
        "- Produktionssysteme wie **MES** (Manufacturing Execution Systems) oder **BDE** (Betriebsdatenerfassung) k√∂nnten als praxisnahe Schnittstelle zur RL-Integration dienen.\n",
        "\n",
        "\n",
        "**Vorteil:**  \n",
        "Reale Produktionsdaten flie√üen in adaptive Pr√ºfstrategien ‚Üí vollautomatisiertes Pr√ºfsystem mit R√ºckkopplung\n",
        "\n",
        "---\n",
        "\n",
        "**N√§chster Schritt:**  \n",
        " Integration eines DQN mit Convolutional/Linear Layers zur **Generalisation √ºber Zustandsr√§ume hinweg**\n",
        "\n",
        "**Langfristige Perspektive:**\n",
        "- Einbindung von Sensordaten (z.‚ÄØB. reale Temperaturverl√§ufe, Korrosionsraten)  \n",
        "- SAP-Datenanbindung zur intelligenten Pr√ºfsteuerung basierend auf Echtzeitdaten  \n",
        "- Einsatz von Transfer Learning, um Wissen von einem Lagertyp auf andere Varianten zu √ºbertragen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  Kontakt & Profile\n",
        "\n",
        "\n",
        "-  LinkedIn (https://www.linkedin.com/in/baris-enes)\n",
        "-  GitHub: (https://github.com/baris-enes)\n",
        "\n",
        "---\n",
        "\n",
        "*Dieses Projekt wurde im Rahmen meiner Spezialisierung auf Reinforcement Learning f√ºr die industriellen Qualit√§tssicherung entwickelt ‚Äì mit besonderem Fokus auf physikalisch fundierte Entscheidungsmodelle und SAP-nahe Produktionsdaten.*\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3_11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
