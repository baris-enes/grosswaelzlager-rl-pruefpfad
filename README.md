#  Reinforcement Learning basierte PrÃ¼fpfadentscheidung bei Offshore-WÃ¤lzlagern

## Projektziel:

### Der Reinforcement-Learning-Agent soll lernen, fÃ¼r ein Offshore-GroÃŸwÃ¤lzlager auf Basis des aktuellen Zustands (Temperaturgradient, WÃ¤rmebehandlungsstatus, Aktionshistorie) eine geeignete PrÃ¼fstrategie auszuwÃ¤hlen. Dabei steht die Entscheidung im Fokus, ob:

- keine PrÃ¼fung (Skip),

- eine kostengÃ¼nstige Brinell-HÃ¤rteprÃ¼fung oder

- eine aufwendige UltraschallprÃ¼fung (UT) erforderlich ist, und ob vorherige ZustandsverÃ¤nderungen (z.â€¯B. Heizen, WÃ¤rmebehandlung) sinnvoll sind, um eine kosteneffizientere PrÃ¼fung zu ermÃ¶glichen.

##  Physikalischer Hintergrund

**Werkstoff:** 100Cr6 (1.3505), typischer WÃ¤lzlagerstahl  
**HÃ¤rte nach WÃ¤rmebehandlung:** ~**700 HBW** (â‰ˆ 62 HRC, umgerechnet)  
**Einsatzbereich:** Offshore-Anwendungen mit **C5-M**-Korrosionsanforderung gemÃ¤ÃŸ **ISO 12944**

###  Wichtige Werkstoffmerkmale:
- Hohe HÃ¤rte und VerschleiÃŸfestigkeit nach Ã–labschrecken und Anlassen
- Nicht korrosionsbestÃ¤ndig â†’ **Chromgehalt < 13â€¯%**
- ZusÃ¤tzlicher **Korrosionsschutz erforderlich** (z.â€¯B. Zn/Ni-Beschichtung, Kapselung, Dichtungen)


###  PrÃ¼fmethoden im Vergleich

| Verfahren        | Zeitaufwand      | Eigenschaften                             | Typischer Einsatz                    |
|------------------|------------------|--------------------------------------------|--------------------------------------|
| **Brinell (HBW)**| ~**3 Minuten**    | mechanisch robust, flÃ¤chige Eindrucksmessung | Standardkontrolle fÃ¼r gehÃ¤rtete OberflÃ¤chen |
| **Ultraschall (UT1 + UT2)** | ~**30 Minuten** | fehlerortend, tiefenauflÃ¶send, teurer     | bei Verdacht auf innere Defekte oder Endkontrolle |

**Entscheidungskriterium:**  
Der Reinforcement-Learning-Agent soll **selbststÃ¤ndig lernen**, ob eine kostengÃ¼nstige Brinell-PrÃ¼fung **ausreicht**, oder ob eine aufwendige UltraschallprÃ¼fung **nÃ¶tig ist** â€“ je nach Zustand des Bauteils.


###  Thermische Gradientendynamik

**Temperaturverteilung innerhalb des GroÃŸwÃ¤lzlagers** beeinflusst die Eigenspannungen und PrÃ¼fentscheidungen:

<p align="center">
  <img src="plots/Temperatur_gradient.png" alt="Temperaturgradient" width="470"/>
</p>

###  WÃ¤rmebehandlungsverlauf

Umwandlungen wie Austenit â†’ Martensit oder Bainit folgen temperaturabhÃ¤ngigen Reaktionsraten, modelliert durch die Arrhenius-Gleichung:

$$
k(T) = A \cdot \exp\left(-\frac{Q}{R \cdot T}\right)
$$

**Legende:**

- \(k(T)\): Reaktionsgeschwindigkeit  
- \(A\): prÃ¤exponentieller Faktor  
- \(Q\): Aktivierungsenergie [J/mol]  
- \(R = 8,314~{J/molÂ·K}\): universelle Gaskonstante  
- \(T\): Temperatur in Kelvin



## Spielregeln

### Zustand

- temperaturgradient âˆˆ [0, 1]: simuliert âˆ‡T normiert, kritisch ab ~0.7
- wÃ¤rmebehandlungsgrad âˆˆ {0, 1}: 1 = korrekt durchgefÃ¼hrt, 0 = mangelhaft
- last_action, prev_action âˆˆ {0â€“5} (Index: [Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung])


###  AktionsÃ¼bersicht & Rewards

| Index | Aktion                  | Effekt                                            | Reward (wenn erlaubt) | Reward (wenn nicht erlaubt)                      |
|-------|-------------------------|---------------------------------------------------|------------------------|--------------------------------------------------|
| 0     | Skip                    | Keine PrÃ¼fung, beendet Episode                   | 0                      | âˆ’5 (wenn âˆ‡T > 0.7 oder WBH < 0.3)                |
| 1     | UT (UltraschallprÃ¼fung) | beendet Episode                                  | +3                     | âˆ’1.5                                             |
| 2     | Brinell (HÃ¤rteprÃ¼fung)  | beendet Episode                                  | +2 (wenn WBH â‰¥ 0.3)    | âˆ’1 (wenn WBH < 0.3)                              |
| 3     | Heizen                  | âˆ‡T + 0.1, kein done                               | âˆ’0.1                   | â€”                                                |
| 4     | KÃ¼hlen                  | âˆ‡T âˆ’ 0.1, WBH âˆ’ 0.1, kein done                    | âˆ’0.1                   | â€”                                                |
| 5     | WÃ¤rmebehandlung         | WBH + 0.2, kein done                              | âˆ’0.1                   | â€”                                                |


### Entscheidungslogik

- Wenn `temperaturgradient > 0.7` â†’ **UT ist sicherer**  
  (hohe Eigenspannung â†’ Brinell ungeeignet)

- Wenn `temperaturgradient < 0.5` **und** `wÃ¤rmebehandlungsgrad â‰¥ 0.3` â†’ **Brinell genÃ¼gt**  
  (kosteneffiziente Variante, maximaler Reward)

- Wenn `wÃ¤rmebehandlungsgrad < 0.3` â†’ **Brinell nicht erlaubt**  
  â†’ Agent kann:
  - entweder `UT` durchfÃ¼hren (sicher, aber teuer), **oder**
  - vorher `wÃ¤rmebehandeln`, um `wÃ¤rmebehandlungsgrad` auf â‰¥ 0.3 zu bringen und **dann Brinell**

- Aktionen wie `Heizen`, `KÃ¼hlen`, `WÃ¤rmebehandlung` dienen dazu, den Zustand gezielt in einen prÃ¼fbaren Bereich zu verschieben  
  (z.â€¯B. âˆ‡T erhÃ¶hen fÃ¼r UT, WÃ¤rmebehandlung verbessern fÃ¼r Brinell)

- âš ï¸ **Um zu verhindern, dass der Agent durch wiederholtes Heizen kÃ¼nstlich den UT-Zustand erzeugt, um konstant +3 zu kassieren,** wurde eine Regel eingefÃ¼hrt:
- **Maximal 2 Aktionen pro Episode**
- **Wiederholung derselben Aktion fÃ¼hrt zu einer Strafbewertung von âˆ’1.0**


**Ziel des Agenten:**
- So **wenig UT wie nÃ¶tig** (teuer, aber sicher)
- So **wenig Skip wie mÃ¶glich** (QualitÃ¤tsrisiko)
- So **oft wie mÃ¶glich Brinell**, **wenn Zustand es zulÃ¤sst**
- **ZustandsverÃ¤nderungen intelligent nutzen**, um gÃ¼nstige PrÃ¼fungen zu ermÃ¶glichen


 ###  <h3 align="center">Flowchart - Entscheidungslogik</h3>
 <p align="center">
  <img src="plots/rl_flowchart_.png" width="450"/>
</p>




### Beispielhafte Entscheidungslogik eines intelligenten Agents

####  **Szenario 1: Direkter Brinell-Check**
- Zustand: `temperaturgradient = 0.3`, `wÃ¤rmebehandlungsgrad = 0.9`
- Aktion: `Brinell` (gÃ¼nstig, schnell)
- Reward: **+2**

 Entscheidung: Der Agent erkennt einen idealen Zustand und nutzt die **kostengÃ¼nstige Brinell-PrÃ¼fung**.

---

####  **Szenario 2: Unsicherer Zustand â†’ Zustand verbessern â†’ Brinell**
- Anfangszustand: `temperaturgradient = 0.4`, `wÃ¤rmebehandlungsgrad = 0.1`
- Aktionen: Brinell nicht erlaubt, da WÃ¤rmebehandlung die Mindestanforderung von 0.3 nicht erreicht
  1. `WÃ¤rmebehandlung` â†’ `wÃ¤rmebehandlungsgrad` steigt auf 0.3 â†’ Reward: **âˆ’0.1**
  2. `Brinell` (nun erlaubt) â†’ Reward **+2**

- Gesamt-Reward: **+1.9**

 Entscheidung: Der Agent investiert **einmalig Energie**, um einen Brinell-PrÃ¼fzustand herzustellen. **GÃ¼nstiger als UT.**

---

####  **Szenario 3: Temperatur zu niedrig â†’ Agent heizt â†’ dann UT**
- Anfangszustand: `temperaturgradient = 0.4`, `wÃ¤rmebehandlungsgrad = 0.8`
- Aktionen:
  1. `Heizen` â†’ `temperaturgradient = 0.5` â†’ Reward: **âˆ’0.1**
  2. `UT` â†’ Reward: **+3**

- Gesamt-Reward: **+2.9**

##  Ãœberblick der RL-Umgebungen (Environments)

### ğŸŸ¡ `WzlPruefEnv2D` 
- Zustandsraum:
`temperaturgradient` (z.â€¯B. innen â†” auÃŸen)
`wÃ¤rmebehandlungsgrad` (QualitÃ¤t des WÃ¤rmeprozesses)
- `state = (temperaturgradient, wÃ¤rmebehandlungsgrad)`


- Ziel:
Lerne einfache PrÃ¼fentscheidungen basierend auf aktuellem Zustand.

- Typische Strategie:
â†’ â€Wenn gradient hoch â†’ UT; wenn Behandlung schlecht â†’ Brinellâ€œ

- Einsatz:
Baseline-Agenten wie SimpleAgentV1, erstes Q-Learning (V1)


### ğŸŸ  `WzlPruefEnv3D`
- ZusÃ¤tzlicher Zustand:
last_action â€“ Was wurde in der vorherigen Aktion gemacht?
- `state = (temperaturgradient, wÃ¤rmebehandlungsgrad, last_action)`

- Ziel:
Modellieren von Verkettungen wie:
â€Heizen â†’ anschlieÃŸend Brinellâ€œ oder â€WÃ¤rmebehandlung â†’ UTâ€œ

- Warum wichtig?
PrÃ¼fpfade sind oft mehrstufig, nicht isolierte Einzelschritte.

- Einsatz:
QLearningAgentV2 (mit rudimentÃ¤rem Entscheidungsverlauf)

### ğŸ”µ`WzlPruefEnv4D`
- ZusÃ¤tzlicher Zustand:
prev_action â€“ Zwei Aktionen zurÃ¼ck (wie ein kurzes GedÃ¤chtnis)
- `state = (temperaturgradient, wÃ¤rmebehandlungsgrad, last_action, prev_action)`

- Ziel:
RÃ¼ckkopplungseffekte, PrÃ¼fpfad-Optimierung Ã¼ber mehrere Schritte hinweg

- Herausforderung:
10Ã—10Ã—6Ã—6 ZustÃ¤nde = 3600 EintrÃ¤ge â†’ Q-Table stÃ¶ÃŸt an Grenzen

- Einsatz:
 DQN â€“ notwendig fÃ¼r tieferes EntscheidungsverstÃ¤ndnis

##  Agentenentwicklung â€“ Evolution der Strategien

###  SimpleAgentV1

- **Typ:** Regelbasierter Agent ohne Zustandsverfolgung  
- **Zustand:** `(Temperaturgradient, WÃ¤rmebehandlungsgrad)`  
- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  
- **Reward:** Ã˜ ca. **0.84**
- **Erkenntnis:**  
  - Klare Entscheidungsregeln schlagen teilweise explorative Lernmethoden  
  - Diente als robuster Benchmark fÃ¼r spÃ¤tere Agenten

  ###  <h3 align="center">Lernverlauf â€“ SimpleAgentV1</h3>

<p align="center">
  <img src="plots/SimpleAgentV1_.png" width="800"/>
</p>



###  QLearningAgentV1 (2D)

- **Zustand:** `(Temperaturgradient, WÃ¤rmebehandlungsgrad)`  
- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  
- **Reward:** Ã˜ bis **~1.43**  
- **Besonderheit:**  
  - Klassischer Q-Learning Agent mit tabellarischer ReprÃ¤sentation  
  - Grid Search Ã¼ber Î±, Î³, Îµ_min und decay  
  - Visualisierung der besten Kombinationen (Top 1â€“3â€“5â€“10) mit Moving Average

- **Erkenntnisse:**  
  - Grid Search zeigte starke HyperparameterabhÃ¤ngigkeit  
  - Einfache Umgebung â†’ schneller Lerneffekt

  ### <h3 align="center">Lernverlauf â€“ QLearningAgentV1 (2D)</h3>

<p align="center">
  <img src="plots/QLearningAgentV1_.png" width="800"/>
</p>


###  QLearningAgentV2 (3D)

- **Zustand:** `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action)`  
- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  
- **Reward:** Ã˜ bis **~1.55**  
- **Ziel:** Modellierung einfacher Sequenzen wie â€Heizen â†’ Brinellâ€œ  
- **Besonderheit:**  
  - Zustandserweiterung um letzte Aktion  
  - 600 mÃ¶gliche ZustÃ¤nde

- **Erkenntnisse:**  
  - Verbesserte Leistung gegenÃ¼ber QLearningAgentV1  
  - Historische Kontextinformation fÃ¼hrt zu gezielteren Entscheidungen  
  - Erste Anzeichen steigender KomplexitÃ¤t â†’ Performance stagniert ab gewissen Grenzen

  ### <h3 align="center">Lernverlauf â€“ QLearningAgentV2 (3D)</h3>

<p align="center">
  <img src="plots/QLearningAgentV2_.png" width="800"/>
</p>

###  DQN (4D, mit neuronalen Netzen)

- **Zustand:** `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action, prev_action)`  
- **Aktionen:** 6 (Skip, UT, Brinell, Heizen, KÃ¼hlen, WÃ¤rmebehandlung)  
- **Reward:** Plateau bei **2.03**, auch nach 200k Episoden  
- **Besonderheit:**  
  - Deep Q-Network auf Basis von TensorFlow  
  - 3600 mÃ¶gliche ZustÃ¤nde â†’ tabellarisches Q-Learning nicht mehr sinnvoll   
  - Speichert Rewards automatisch in `.npy`-Dateien â†’ resume-fÃ¤hig

- **Erkenntnisse:**  
  - Klassisches Q-Learning skaliert schlecht in hohen Dimensionen  
  - DQN ermÃ¶glicht bessere Verallgemeinerung, aber Reward bleibt limitiert  
  - Hauptlimitierender Faktor: das physikalisch definierte Reward-System

  ### <h3 align="center">Lernverlauf â€“ DQNAgent (4D) </h3> 
  <p align="center">
  <img src="plots/DQNAgent_.png" width="800"/>
</p>




[def]: lots/SimpleAgentV1_.pn

##  Projekttechnik

- **Grid Search** Ã¼ber >50 Hyperparameterkombinationen (`Î±`, `Î³`, `Îµ_min`, `decay`)
- **Automatische Zwischenspeicherung** aller Trainingskurven (`.npy`) fÃ¼r Reproduzierbarkeit
- **Logging**: Durchschnittlicher Reward alle 1000 Episoden zur Verlaufskontrolle
- **Visualisierung** der Top-N-Strategien mit:
  - **Moving Average** (FenstergrÃ¶ÃŸe: 100)
  - **Farbkodierung** (Top 1â€“3: blau, rot, orange)
  - **Export** als `.csv` und `.png` fÃ¼r weitere Analyse
- **DQN-Modul** (Deep Q-Network) fÃ¼r Zustandsverarbeitung mit neuronalen Netzen
â†’ Anwendung von Replay Buffer, Îµ-greedy Policy, Target Network

###  Skalierung der RL-Umgebungen

Mit jeder Agentengeneration wurde gezielt die **Zustandsdimensionierung erhÃ¶ht**:

- `2D`: Basiszustand  
  â†’ `(Temperaturgradient, wÃ¤rmebehandlungsgrad)`
- `3D`: zusÃ¤tzlicher Kontext Ã¼ber letzte Aktion  
  â†’ `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action)`
- `4D`: komplette Entscheidungsfolge Ã¼ber zwei Zeitschritte  
  â†’ `(Temperaturgradient, wÃ¤rmebehandlungsgrad, last_action, prev_action)`

 **Ziel**: Bewertung, wie weit klassische RL-Verfahren (Q-Table, DQN) mit wachsender ZustandskomplexitÃ¤t skalieren.




##  Verzeichnisstruktur

```plaintext
grosswaelzlager_pruef_rl/
â”œâ”€â”€ README.ipynb                  â† Hauptdokumentation mit RL- und Werkstoffwissen
â”œâ”€â”€ requirements.txt              â† AbhÃ¤ngigkeiten (pip install)
â”œâ”€â”€ plots/                        â† Trainingskurven & Flowcharts
â”‚   â”œâ”€â”€ DQNAgent_.png
â”‚   â”œâ”€â”€ QLearningAgentV1_.png
â”‚   â”œâ”€â”€ QLearningAgentV2_.png
â”‚   â”œâ”€â”€ SimpleAgentV1_.png
â”‚   â””â”€â”€ rl_flowchart_.png
â”œâ”€â”€ environments/                â† Simulationsumgebungen (2D, 3D, 4D)
â”‚   â”œâ”€â”€ env_wzl_0.py             2D
â”‚   â”œâ”€â”€ env_wzl_1.py             3D
â”‚   â””â”€â”€ env_wzl_2.py             4D
â”œâ”€â”€ rl_agent/                    â† Verschiedene Agenten-Implementierungen
â”‚   â”œâ”€â”€ agent_simple_v1.py
â”‚   â”œâ”€â”€ DQNAgent.py
â”‚   â”œâ”€â”€ q_learning_agent_V1.py
â”‚   â””â”€â”€ q_learning_agent_V2.py
â”œâ”€â”€ notebooks/                   â† Auswertung und Visualisierung
â”‚   â””â”€â”€ agenten_plots.ipynb

```


##  Fazit & nÃ¤chste Schritte

- Klassische **Q-Learning-Tabellen** reichen fÃ¼r einfache, klar strukturierte PrÃ¼fentscheidungen aus  
- In **hochdimensionalen ZustandsrÃ¤umen**,  (z.â€¯B. `env_wzl_2 - 4D`) ist eine **Deep-Q-Network-Architektur (DQN)** notwendig  
- Die Kombination aus **DomÃ¤nenwissen** (_z.â€¯B. Gradientformel, Werkstoffverhalten_) und datengetriebenem Lernen liefert robuste PrÃ¼fstrategien  
  Nach 200.000 Episoden erreichte er einen stabilen durchschnittlichen Reward von   **Ã˜ 2.05**.
  

---

###  Industrie 4.0 â€“ SAP-Anbindung (Optionaler Ausblick)

- Ãœber ein SAP S/4HANA-System lassen sich produktionsnahe Parameter wie WÃ¤rmebehandlungsstatus, PrÃ¼fauftragsdaten oder Materialchargen automatisiert bereitstellen
- Das RL-Modell kann so auf **Live-Daten** reagieren und direkt in **QualitÃ¤tsentscheidungen** integriert werden (z.â€¯B. Ã¼ber das Modul **SAP QM** oder **SAP PP**)
- Einbindung Ã¼ber **OData- oder REST-API** wÃ¤re technisch realisierbar (z.â€¯B. via Python-Schnittstelle zu SAP Gateway)
- Produktionssysteme wie **MES** (Manufacturing Execution Systems) oder **BDE** (Betriebsdatenerfassung) kÃ¶nnten als praxisnahe Schnittstelle zur RL-Integration dienen.


**Vorteil:**  
Reale Produktionsdaten flieÃŸen in adaptive PrÃ¼fstrategien â†’ vollautomatisiertes PrÃ¼fsystem mit RÃ¼ckkopplung

---

**NÃ¤chster Schritt:**  
 Integration eines DQN mit Convolutional/Linear Layers zur **Generalisation Ã¼ber ZustandsrÃ¤ume hinweg**

**Langfristige Perspektive:**
- Einbindung von Sensordaten (z.â€¯B. reale TemperaturverlÃ¤ufe, Korrosionsraten)  
- SAP-Datenanbindung zur intelligenten PrÃ¼fsteuerung basierend auf Echtzeitdaten  
- Einsatz von Transfer Learning, um Wissen von einem Lagertyp auf andere Varianten zu Ã¼bertragen



##  Kontakt & Profile


-  LinkedIn (https://www.linkedin.com/in/baris-enes)
-  GitHub: (https://github.com/baris-enes)

---

*Dieses Projekt wurde im Rahmen meiner Spezialisierung auf Reinforcement Learning fÃ¼r die industriellen QualitÃ¤tssicherung entwickelt â€“ mit besonderem Fokus auf physikalisch fundierte Entscheidungsmodelle und SAP-nahe Produktionsdaten.*

